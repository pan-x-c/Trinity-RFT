diff --git a/trinity/trainer/verl/fsdp_workers.py b/trinity/trainer/verl/fsdp_workers.py
index b82fd264..0fff4233 100644
--- a/trinity/trainer/verl/fsdp_workers.py
+++ b/trinity/trainer/verl/fsdp_workers.py
@@ -610,7 +610,7 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):

     @register(dispatch_mode=Dispatch.ONE_TO_ALL)
     def init_model(self):
-        from trinity.trainer.verl.dp_actor import DataParallelPPOActor
+        from examples.entropy.clipv_dp_actor import DataParallelPPOActor

         # This is used to import external_lib into the huggingface systems
         import_external_libs(self.config.model.get("external_lib", None))
@@ -911,6 +911,8 @@ class ActorRolloutRefWorker(Worker, DistProfilerExtension):
                 tensors = {"ref_log_prob": outputs["log_probs"]}
             if calculate_entropy:
                 tensors["entropys"] = outputs["entropys"]
+            if "necs" in outputs:
+                tensors["necs"] = outputs["necs"]
             output = DataProto.from_dict(
                 tensors=tensors,
                 meta_info={"temperature": self.config.rollout.temperature},
diff --git a/trinity/trainer/verl_trainer.py b/trinity/trainer/verl_trainer.py
index 849e176b..734b8b26 100644
--- a/trinity/trainer/verl_trainer.py
+++ b/trinity/trainer/verl_trainer.py
@@ -24,13 +24,15 @@ from verl.trainer.ppo.ray_trainer import (
     Role,
     create_colocated_worker_cls,
 )
+from verl import DataProto
+from verl.utils import tensordict_utils as tu
+from verl.workers.utils.padding import left_right_2_no_padding, no_padding_2_padding
 from verl.utils import hf_processor, hf_tokenizer
 from verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path
 from verl.utils.debug import marked_timer
 from verl.utils.fs import copy_local_path_from_hdfs
 from verl.utils.metric import reduce_metrics
 from verl.workers.config import FSDPEngineConfig
-
 from trinity.algorithm import ADVANTAGE_FN, ALGORITHM_TYPE, KL_FN
 from trinity.algorithm.utils import prefix_metrics
 from trinity.common.config import Config
@@ -433,6 +435,37 @@ class VerlPPOTrainerWrapper(RayPPOTrainer, TrainEngineWrapper):
         self.config.actor_rollout_ref.actor.optim.total_training_steps = self.total_training_steps
         self.config.critic.optim.total_training_steps = self.total_training_steps

+    def _compute_old_log_prob(self, batch: DataProto):
+        """
+        We add nec to the batch to make the advantage function (e.g. Clip_V) use it.
+        """
+        if self.use_legacy_worker_impl == "disable":
+            # TODO: remove step 1, 2, 4 after we make the whole training tensordict and padding free
+            # step 1: convert dataproto to tensordict.
+            batch_td = batch.to_tensordict()
+            # step 2: convert from padding to nopadding
+            batch_td = left_right_2_no_padding(batch_td)
+            # step 3: add meta info
+            tu.assign_non_tensor(batch_td, calculate_entropy=True, compute_loss=False)
+            output = self.actor_rollout_wg.compute_log_prob(batch_td)
+            # gather output
+            entropy = tu.get(output, "entropy")
+            log_probs = tu.get(output, "log_probs")
+            old_log_prob_mfu = tu.get(output, "metrics")["mfu"]
+            necs = tu.get(output, "necs")
+            # step 4. No padding to padding
+            entropy = no_padding_2_padding(entropy, batch_td)
+            log_probs = no_padding_2_padding(log_probs, batch_td)
+            necs = no_padding_2_padding(necs, batch_td)
+            # step 5: rebuild a tensordict and convert to dataproto
+            old_log_prob = tu.get_tensordict({"old_log_probs": log_probs.float(), "entropys": entropy.float(), "necs": necs.float()})
+            old_log_prob = DataProto.from_tensordict(old_log_prob)
+        else:
+            old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
+            old_log_prob_mfu = 0
+        return old_log_prob, old_log_prob_mfu
+
+
     async def save_state_dict(self):  # checkpoint sync
         actor_local_path = os.path.join(
             self.config.trainer.default_local_dir, f"global_step_{self.global_steps}", "actor"
@@ -501,13 +534,17 @@ class VerlPPOTrainerWrapper(RayPPOTrainer, TrainEngineWrapper):
                         "perf/mfu/actor_infer": old_log_prob_mfu,
                     }
                     metrics.update(old_log_prob_metrics)
-                    old_log_prob.batch.pop("entropys")
+                    # old_log_prob.batch.pop("entropys")
                     batch = batch.union(old_log_prob)
                     if "rollout_log_probs" in batch.batch.keys():
                         # TODO: we may want to add diff of probs too.
                         from verl.utils.debug.metrics import calculate_debug_metrics

                         metrics.update(calculate_debug_metrics(batch))
+
+                    batch.batch["new_log_probs"] = old_log_prob.batch["old_log_probs"]
+                    batch.batch["new_entropys"]  = entropys
+                    batch.batch["necs"] = old_log_prob.batch["necs"]

             if self.algorithm.use_reference:  # ref_logprob may not be used
                 # compute reference log_prob
@@ -526,7 +563,8 @@ class VerlPPOTrainerWrapper(RayPPOTrainer, TrainEngineWrapper):
                     batch, kl_metrics = self.kl_fn.apply_kl_penalty_to_reward(batch)
                     metrics.update(prefix_metrics(kl_metrics, prefix="critic"))
                     # compute advantages, executed on the driver process
-                    batch, _ = self.advantage_fn(batch)
+                    batch, adv_metrics = self.advantage_fn(batch)
+                    metrics.update(prefix_metrics(adv_metrics, prefix="clipv"))
             else:
                 # skip token_level_scores for sft/dpo
                 if "token_level_scores" in batch.batch.keys():
